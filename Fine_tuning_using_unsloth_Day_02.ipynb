{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riyajidevindu/Advanced-AI-Codes/blob/main/Fine_tuning_using_unsloth_Day_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0913be9-d9d6-425b-98d1-e2a5771bcde0",
      "metadata": {
        "id": "a0913be9-d9d6-425b-98d1-e2a5771bcde0"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Unsloth and other required libraries\n",
        "!pip install -U unsloth trl bitsandbytes datasets accelerate\n",
        "# Installs the latest versions of Unsloth, TRL (for training), bitsandbytes (for quantized models), datasets (for loading datasets), and accelerate (for fast training)\n",
        "\n",
        "# Login to Hugging Face (necessary to access gated models)\n",
        "!huggingface-cli login\n",
        "# Prompts user to log in to their Hugging Face account via access token\n",
        "\n",
        "# Step 2: Load Model via Unsloth\n",
        "from unsloth import FastLanguageModel  # Imports Unsloth's core wrapper for fast LLM training\n",
        "import torch                           # Imports PyTorch for tensor operations and device management\n",
        "from datasets import Dataset           # Imports Hugging Face's dataset loading utility\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-1.1-2b-it-bnb-4bit\",  # Loads a 4-bit quantized version of Gemma 2B IT model optimized for Unsloth\n",
        "    max_seq_length = 2048,                            # Sets the maximum token length for model input\n",
        "    dtype = torch.float16,                            # Uses float16 precision to save memory and accelerate computation\n",
        "    load_in_4bit = True,                              # Loads the model in 4-bit quantized mode (saves even more memory)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "3na0bXqpw-u0"
      },
      "id": "3na0bXqpw-u0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"### Instruction: Analyze the sentiment\\n### Input: I love programming .\\n### Response:\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "Od9jdgy_xDit"
      },
      "id": "Od9jdgy_xDit",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft.tuners.lora import LoftQConfig\n",
        "\n",
        "# Now prepare model for training (adds LoRA adapters)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,                                            # Passes the base model\n",
        "    r = 8,                                            # Rank of the LoRA adapters (controls capacity of the adaptation). Controls how many low-rank dimensions are added for adaptation.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],  # Specifies which model layers will receive LoRA adapters. List of transformer modules to inject LoRA adapters into.\n",
        "    lora_alpha = 16,                                  # Scaling factor for the LoRA update. Think of this as a learning rate multiplier for the LoRA part.\n",
        "    lora_dropout = 0,                                 # No dropout is used (recommended for very small datasets)\n",
        "    bias = \"none\",                                    # Does not adapt bias terms in the model. LoRA does not adapt the bias terms in the model layers, saving compute and memory.\n",
        "    use_gradient_checkpointing = True,                # Enables gradient checkpointing to reduce memory usage during training. Reduces memory usage during training by recomputing activations in the backward pass. Slows down training but helps fit larger models on smaller GPUs.\n",
        "    random_state = 3407,                              # Sets seed for reproducibility\n",
        "    use_rslora = True,                               # Disables rank-splitting LoRA (optional optimization).Disables Rank-Splitting LoRA, which is a variation that can improve stability. Off by default.\n",
        "    loftq_config = LoftQConfig(loftq_bits=4)   # No QLoRA-specific configuration used. Not using QLoRA-specific optimizations (like quantization-aware LoRA). Can be used for extra memory savings.\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_training(model)  # Prepares model for training (enables gradient computation, checkpointing, etc.)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nOfajLPpWtP",
        "outputId": "7e104506-b155-403c-9a0e-a8049a5da0e8"
      },
      "id": "4nOfajLPpWtP",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/config.py:576: UserWarning: `loftq_config` specified but will be ignored when `init_lora_weights` is not 'loftq'.\n",
            "  warnings.warn(\"`loftq_config` specified but will be ignored when `init_lora_weights` is not 'loftq'.\")\n",
            "Unsloth 2025.6.2 patched 18 layers with 18 QKV layers, 18 O layers and 18 MLP layers.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GemmaForCausalLM(\n",
              "      (model): GemmaModel(\n",
              "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-17): 18 x GemmaDecoderLayer(\n",
              "            (self_attn): GemmaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): GemmaFixedRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): GemmaMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=16384, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): PytorchGELUTanh()\n",
              "            )\n",
              "            (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "            (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "        (rotary_emb): GemmaFixedRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: your 3 documents formatted for instruction fine-tuning\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"### Instruction: Analyze the sentiment\\n### Input: I love programming .\\n### Response: Positive\",\n",
        "        \"### Instruction: Analyze the sentiment\\n### Input: Programming is fun .\\n### Response: Positive\",\n",
        "        \"### Instruction: Analyze the sentiment\\n### Input: I love fun activities activities.\\n### Response: Positive\",\n",
        "        \"### Instruction: Analyze the sentiment\\n### Input: Worst movie ever.\\n### Response: Negative\",\n",
        "        \"### Instruction: Analyze the sentiment\\n### Input: This is satifactory despite negative features.\\n### Response: Neutral\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data)\n"
      ],
      "metadata": {
        "id": "yi5Ik1sgYyvu"
      },
      "id": "yi5Ik1sgYyvu",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    packing = True,\n",
        "    args = TrainingArguments(\n",
        "        output_dir = \"./unsloth_gemma_finetuned\",\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        num_train_epochs = 2,\n",
        "        logging_steps = 10,\n",
        "        save_steps = 50,\n",
        "        save_total_limit = 2,\n",
        "        bf16 = False,\n",
        "        fp16 = True,\n",
        "        optim = \"adamw_8bit\",\n",
        "        warmup_steps = 5,\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        learning_rate = 2e-4,\n",
        "        report_to = \"none\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "sNziyp5GY5bc"
      },
      "id": "sNziyp5GY5bc"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from trl import SFTTrainer\n",
        "# Imports SFTTrainer from the TRL library, used for supervised fine-tuning (SFT)\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "# Imports Hugging Face's TrainingArguments class to specify training configuration\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    # Specifies the pre-loaded PEFT model (e.g., Unsloth-wrapped Gemma)\n",
        "\n",
        "    tokenizer = tokenizer,\n",
        "    # Specifies the tokenizer corresponding to the model\n",
        "\n",
        "    train_dataset = dataset,\n",
        "    # Specifies the training dataset\n",
        "\n",
        "    dataset_text_field = \"text\",\n",
        "    # Tells the trainer which field in the dataset contains the input text\n",
        "\n",
        "    max_seq_length = 2048,\n",
        "    # Sets the maximum sequence length for training samples\n",
        "\n",
        "    packing = True,\n",
        "    # Enables sequence packing for efficient use of token space (fills batches with multiple samples when possible)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir = \"./unsloth_gemma_finetuned\",\n",
        "        # Directory where model checkpoints and logs will be saved\n",
        "\n",
        "        per_device_train_batch_size = 1,\n",
        "        # Sets batch size per GPU (Colab has one GPU, so this is total batch size per step)\n",
        "\n",
        "        gradient_accumulation_steps = 4,\n",
        "        # Accumulates gradients over 4 steps before updating weights (simulates larger batch size)\n",
        "\n",
        "        num_train_epochs = 2,\n",
        "        # Trains for 2 full passes over the training dataset\n",
        "\n",
        "        logging_steps = 10,\n",
        "        # Logs training metrics every 10 steps\n",
        "\n",
        "        save_steps = 50,\n",
        "        # Saves a checkpoint every 50 training steps\n",
        "\n",
        "        save_strategy=\"steps\",\n",
        "\n",
        "        save_total_limit = 2,\n",
        "        # Keeps only the 2 most recent checkpoints, deleting older ones\n",
        "\n",
        "        bf16 = False,\n",
        "        # Disables bfloat16 training (not used here)\n",
        "\n",
        "        fp16 = True,\n",
        "        # Enables mixed-precision training using float16 for speed and memory efficiency on GPUs\n",
        "\n",
        "        optim = \"adamw_8bit\",\n",
        "        # Uses 8-bit AdamW optimizer from bitsandbytes to reduce memory usage\n",
        "\n",
        "        warmup_steps = 5,\n",
        "        # Number of warmup steps for learning rate scheduler\n",
        "\n",
        "        weight_decay = 0.01,\n",
        "        # Applies weight decay (L2 regularization) to reduce overfitting\n",
        "\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        # Uses a linear learning rate scheduler\n",
        "\n",
        "        learning_rate = 2e-4,\n",
        "        # Sets the base learning rate for the optimizer\n",
        "\n",
        "        report_to = \"none\",\n",
        "        # Disables integration with external logging tools (e.g., WandB, TensorBoard),\n",
        "\n",
        "    ),\n",
        "\n",
        ")\n",
        "# Creates the trainer object for supervised fine-tuning using all the above configuration\n",
        "\n",
        "trainer.train()\n",
        "# Starts the training process\n",
        "\n",
        "# Save your fine-tuned PEFT model\n",
        "from peft import PeftModel\n",
        "# `model` is your PEFT-wrapped (LoRA) model\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(\"unsloth_gemma_finetuned\")\n",
        "tokenizer.save_pretrained(\"unsloth_gemma_finetuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874,
          "referenced_widgets": [
            "da4590da954e43dead0a2bb716e73306",
            "04818204c4f44a03a87460ca7403ead0",
            "372ed1252e754e4e8335e0db83fd6e42",
            "e7a53bd7e5da44a0b6e200f3226981f1",
            "c2ee6b27c461442d878effac28c403c6",
            "0a236d814c4d4ecfae0ddaec92e6d97d",
            "3085fcf9f8da4c99bf4d9e5deaba8600",
            "41294d7484924f33add8f48362225de6",
            "17adf684d5d44eb88e59cce23fbba2cf",
            "184a38ffbdb84731a63ed5a3e2a18d1c",
            "7d6ec5426cd448cdbb4207ca40139308"
          ]
        },
        "id": "hqKUqM3xbsDo",
        "outputId": "55f0cdcf-0d96-462a-a55f-cd3c4551c95d"
      },
      "id": "hqKUqM3xbsDo",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da4590da954e43dead0a2bb716e73306"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 5 | Num Epochs = 2 | Total steps = 4\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 9,805,824/2,000,000,000 (0.49% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:02, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('unsloth_gemma_finetuned/tokenizer_config.json',\n",
              " 'unsloth_gemma_finetuned/special_tokens_map.json',\n",
              " 'unsloth_gemma_finetuned/chat_template.jinja',\n",
              " 'unsloth_gemma_finetuned/tokenizer.model',\n",
              " 'unsloth_gemma_finetuned/added_tokens.json',\n",
              " 'unsloth_gemma_finetuned/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model and tokenizer using Unsloth's method\n",
        "# Use the directory where you saved the model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth_gemma_finetuned\", # Load from your saved directory\n",
        "    max_seq_length = 2048, # Should match or be compatible with training\n",
        "    dtype = torch.float16,  # Should match dtype used for saving if possible\n",
        "    load_in_4bit = True,   # Match how you saved or load in 16-bit if saved that way\n",
        "    # Add any other parameters you used during initial loading with Unsloth\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnvskZtQezJ6",
        "outputId": "83c0f5a2-cf6e-40a3-ec22-8c4c4a3744c1"
      },
      "id": "lnvskZtQezJ6",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.6.2: Fast Gemma patching. Transformers: 4.52.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bYap_8kTzq00"
      },
      "id": "bYap_8kTzq00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero shot inference"
      ],
      "metadata": {
        "id": "oEYcvciqkOWV"
      },
      "id": "oEYcvciqkOWV"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"### Instruction: Analyze the sentiment\\n### Input: I love programming .\\n### Response:\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF_uEpdfcGMR",
        "outputId": "9115b1d3-fa71-41d1-f5c4-6547b2cd3332"
      },
      "id": "BF_uEpdfcGMR",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction: Analyze the sentiment\n",
            "### Input: I love programming .\n",
            "### Response: Positive\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "The sentiment expressed in the input is positive. The user expresses genuine enjoyment and enthusiasm towards the activity of programming.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"### Instruction: Analyze the sentiment\n",
        "### Input: This movie is terrible.\n",
        "### Response:\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY1n8AVNkVwc",
        "outputId": "6f041d1a-2622-4589-ad6f-c6bb87165e78"
      },
      "id": "pY1n8AVNkVwc",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction: Analyze the sentiment\n",
            "### Input: This movie is terrible.\n",
            "### Response: The sentiment expressed in the input is negative.\n",
            "\n",
            "### Explanation:\n",
            "- The word \"terrible\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few shot inference"
      ],
      "metadata": {
        "id": "PaRgv4iwka-k"
      },
      "id": "PaRgv4iwka-k"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"### Instruction: Analyze the sentiment\n",
        "### Input: I love programming.\n",
        "### Response: Positive\n",
        "### Input: I hate debugging.\n",
        "### Response: Negative\n",
        "### Input: The interface is very user-friendly.\n",
        "### Response:\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJmaEJXikehA",
        "outputId": "330ee3ec-e806-4a62-f6fc-5862afa6ac4b"
      },
      "id": "mJmaEJXikehA",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction: Analyze the sentiment\n",
            "### Input: I love programming.\n",
            "### Response: Positive\n",
            "### Input: I hate debugging.\n",
            "### Response: Negative\n",
            "### Input: The interface is very user-friendly.\n",
            "### Response: Positive\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "The sentiment analysis reveals a clear difference in the responses to the three inputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JfayAAj429H4"
      },
      "id": "JfayAAj429H4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "anaconda-2024.02-py310",
      "language": "python",
      "name": "conda-env-anaconda-2024.02-py310-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da4590da954e43dead0a2bb716e73306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04818204c4f44a03a87460ca7403ead0",
              "IPY_MODEL_372ed1252e754e4e8335e0db83fd6e42",
              "IPY_MODEL_e7a53bd7e5da44a0b6e200f3226981f1"
            ],
            "layout": "IPY_MODEL_c2ee6b27c461442d878effac28c403c6"
          }
        },
        "04818204c4f44a03a87460ca7403ead0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a236d814c4d4ecfae0ddaec92e6d97d",
            "placeholder": "​",
            "style": "IPY_MODEL_3085fcf9f8da4c99bf4d9e5deaba8600",
            "value": "Unsloth: Tokenizing [&quot;text&quot;]: 100%"
          }
        },
        "372ed1252e754e4e8335e0db83fd6e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41294d7484924f33add8f48362225de6",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17adf684d5d44eb88e59cce23fbba2cf",
            "value": 5
          }
        },
        "e7a53bd7e5da44a0b6e200f3226981f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_184a38ffbdb84731a63ed5a3e2a18d1c",
            "placeholder": "​",
            "style": "IPY_MODEL_7d6ec5426cd448cdbb4207ca40139308",
            "value": " 5/5 [00:00&lt;00:00, 64.41 examples/s]"
          }
        },
        "c2ee6b27c461442d878effac28c403c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a236d814c4d4ecfae0ddaec92e6d97d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3085fcf9f8da4c99bf4d9e5deaba8600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41294d7484924f33add8f48362225de6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17adf684d5d44eb88e59cce23fbba2cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "184a38ffbdb84731a63ed5a3e2a18d1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d6ec5426cd448cdbb4207ca40139308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}